apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  name: wide-ep-llm-d-decode-dk
  labels:
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: Qwen3-30B-A3B-FP8
    llm-d.ai/role: decode
spec:
  replicas: 1
  leaderWorkerTemplate:
    size: 2
    workerTemplate:
      metadata:
        labels:
          llm-d.ai/inferenceServing: "true"
          llm-d.ai/model: Qwen3-30B-A3B-FP8
          llm-d.ai/role: decode
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: kubernetes.io/hostname
                      operator: In
                      values:
                        - 10.0.66.42   # replace with your decode node names
                        - 10.0.73.241 
        dnsPolicy: ClusterFirstWithHostNet
        hostNetwork: true
        serviceAccountName: qwen
        initContainers:
          - name: routing-proxy
            args:
              - --port=8000
              - --vllm-port=8200
              - --connector=nixlv2
              - --zap-log-level=1
              - --secure-proxy=false
              - --enable-prefiller-sampling
            image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.4.0-rc.1
            imagePullPolicy: Always
            ports:
              - containerPort: 8000
                name: sidecar
                protocol: TCP
            resources: {}
            restartPolicy: Always
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 6Gi # roughly 32MB per local DP plus scratch space
          - name: hf-cache
            emptyDir: {}
          - name: jit-cache
            emptyDir: {}
          - name: devinf
            hostPath:
              path: /dev/infiniband
          - name: model-storage
            hostPath:
              path: /mnt/nvme/models
              type: Directory
          - name: aiter-cache
            hostPath:
              path: /mnt/nvme/cache
              type: Directory
        containers:
        - name: vllm
          #image: quay.io/rh-ee-vsundarr/amd-vllm:latest
          image: quay.io/rh-ee-vsundarr/amd-vllm:rebase-dp-eplb-fix
          securityContext:
            privileged: true # FIXME(varun)
            capabilities:
              add:
              - IPC_LOCK
              - CAP_SYS_ADMIN # FIXME (varun)
            runAsGroup: 0
            runAsUser: 0
          imagePullPolicy: Always
          command:
            - /bin/bash
            - -c
          args:
            - |-
              # Clear /dev/shm on start to prevent running out of space when crashes occur
              # https://github.com/llm-d/llm-d/issues/352
              find /dev/shm -type f -delete

              #################
              # RUN vLLM decode worker
              #################
              START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
            

              # --data-parallel-hybrid-lb: Use exernal load balancing across nodes, and internal load balancing within a node
              # --enable-expert-parallel:  Use TPxDP in attention, EP in MoE layers
              # --async-scheduling: Reduce white space between engine steps
              # --enable-dbo:  Dual batch overlap (DBO) overlaps compute with collective communication.
              # --enable-eplb: Expert-parallel load balancing reduces EP load imbalance by replicating heavily-used experts Performance-memory tradeoff: on DeepSeekV3 eplb uses an extra 2GB per redundant expert per GPU Divisibility constraint: num_routed_experts (256 for DSv3) + num_redundant_experts must be divisible by the number of GPUs.


              # TODO (varun) : Investigate DBO enablement
              # TODO (varun) : Use Mistral's default max-model-len
              exec vllm serve \
                /mnt/models/deepseek-ai/DeepSeek-R1-0528 \
                --served-model-name deepseek-ai/DeepSeek-R1-0528 \
                --all2all-backend mori \
                --port 8200 \
                --trust-remote-code \
                --disable-uvicorn-access-log \
                --data-parallel-hybrid-lb \
                --enable-expert-parallel \
                --tensor-parallel-size $TP_SIZE \
                --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
                --data-parallel-size-local $DP_SIZE_LOCAL \
                --data-parallel-address ${LWS_LEADER_ADDRESS} \
                --data-parallel-rpc-port 5555 \
                --data-parallel-start-rank $START_RANK \
                --kv-transfer-config '{
                    "kv_connector": "DecodeBenchConnector",
                    "kv_role": "kv_both",
                    "kv_connector_extra_config": {
                        "fill_mean": 0.015,
                        "fill_std": 0.0
                    }
                }' \
                --enable-eplb \
                --eplb-config '{"window_size":"5000",
                                "step_interval":"50000",
                                "num_redundant_experts":"32",
                                "log_balancedness":"False",
                                "use_async": "True",
                                "num_communication_groups": "4",
                                "communication_experts_batch_size":"32"}' \
                --async-scheduling \
                --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
                --block-size 1 \
                --api-server-count 1 \
                --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-} \
                --gpu-memory-utilization 0.70

          env:
          # VLLM_MOE_DP_CHUNK_SIZE is a tunable parameter.
          # Higher values increase the memory footprint of the hidden states in the MoE layers,
          # which decreases the available memory for KV Cache.
          # Lower values may cause poor performance, especially in load-imbalanced scenarios.
          # The value 384 was chosen to be greater than the concurrency expected to see on any DP rank.
          # If you increase the VLLM_MOE_DP_CHUNK_SIZE above 510 you will also need to increase the
          # default NVSHMEM_QP_DEPTH to be at least 2 * (chunk_size + 1), which will increase the
          # memory NVSHMEM allocates up front.
          # Has no effect when VLLM_ALL2ALL_BACKEND=deepep_high_throughput.

          - name: VLLM_HTTP_TIMEOUT_KEEP_ALIVE
            value: "60"
          - name: VLLM_ENGINE_READY_TIMEOUT_S
            value: "1200"
          - name: VLLM_MOE_DP_CHUNK_SIZE
            value: "384" # vLLM default is 256
          - name: DP_SIZE_LOCAL
            value: "8"
          - name: TP_SIZE
            value: "1"
          - name: TRITON_LIBCUDA_PATH
            value: /usr/lib64
          - name: VLLM_SKIP_P2P_CHECK
            value: "1"
          - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
            value: "1"
          - name: VLLM_USE_DEEP_GEMM
            value: "0"
          - name: NVIDIA_GDRCOPY
            value: enabled
          - name: NVSHMEM_REMOTE_TRANSPORT
            value: ibgda
          - name: NVSHMEM_IB_ENABLE_IBGDA
            value: "true"
          - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
            value: eth0
          - name: GLOO_SOCKET_IFNAME
            value: eth0
          - name: NCCL_SOCKET_IFNAME
            value: eth0
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          # TODO (varun) - the following env var list could be pruned
          - name: ROCSHMEM_HCA_LIST
            value: "mlx5_0,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_7,mlx5_8,mlx5_9"
          - name: ROCSHMEM_BACKEND
            value: "gda"
          - name: ROCSHMEM_GDA_TRAFFIC_CLASS
            value: "41"
          # Nixl
          - name: UCX_NET_DEVICES 
            value: "mlx5_0:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_5:1,mlx5_7:1,mlx5_8:1,mlx5_9:1"
          - name: UCX_IB_GID_INDEX
            value: "3"
          - name: UCX_IB_TRAFFIC_CLASS
            value: "41"
          # MORI
          - name: MORI_RDMA_DEVICES 
            value: "mlx5_0,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_7,mlx5_8,mlx5_9"
          - name: MORI_IB_GID_INDEX
            value: "3"
          - name: NCCL_CUMEM_ENABLE
            value: "0"
          - name: NCCL_IB_TIMEOUT
            value: "22"
          - name: NCCL_IB_SL
            value: "0"
          - name: NCCL_IB_TC
            value: "41"
          - name: NCCL_IB_GID_INDEX
            value: "3"
          - name: NCCL_DEBUG
            value: WARN
          - name: NCCL_IB_QPS_PER_CONNECTION
            value: "1"
          - name: NCCL_IB_SPLIT_DATA_ON_QPS
            value: "0"
          - name: NCCL_P2P_DISABLE 
            value: "0"
          - name: NCCL_IB_HCA
            value: "mlx5_0,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_7,mlx5_8,mlx5_9"
          - name: NCCL_PXN_DISABLE
            value: "0"
          - name: NCCL_NET_PLUGIN
            value: none
          - name: ROCSHMEM_HEAP_SIZE 
            value: "12884901888"

          - name: NCCL_MIN_NCHANNELS
            value: "1"
          - name: NCCL_MAX_NCHANNELS
            value: "2"
          # Force larger, fewer packets (Aggressive Batching)
          - name: NCCL_BUFFSIZE
            value: "4194304"  # 4MB

          - name: VLLM_ROCM_USE_AITER
            value: "1"
          - name: VLLM_ROCM_USE_AITER_MOE
            value: "1"
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP

          # Use cache directories from the mounted volume under one easy to mount
          # root directory.
          - name: CUDA_CACHE_PATH
            value: /var/cache/vllm/cuda
          - name: CCACHE_DIR
            value: /var/cache/vllm/ccache
          - name: VLLM_CACHE_ROOT
            value: /var/cache/vllm/vllm
          - name: FLASHINFER_WORKSPACE_BASE
            value: /var/cache/vllm/flashinfer
          # HuggingFace is likely to be quite a bit larger, give it its own cache
          - name: HF_HUB_CACHE
            value: /var/cache/huggingface
          - name: TORCHINDUCTOR_MAX_AUTOTUNE
            value: "1"

          ports:
          - containerPort: 8200
            name: metrics
            protocol: TCP
          startupProbe:
            httpGet:
              path: /health
              port: metrics
            initialDelaySeconds: 0
            periodSeconds: 1
            timeoutSeconds: 5
            failureThreshold: 2700
          livenessProbe:
            httpGet:
              path: /health
              port: metrics
            periodSeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /v1/models
              port: metrics
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          resources:
            limits:
              ephemeral-storage: 1Ti
              memory: 512Gi
              amd.com/gpu: "8"
            requests:
              cpu: 32
              ephemeral-storage: 1Ti
              memory: 512Gi
              amd.com/gpu: "8"
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: hf-cache
              mountPath: /var/cache/huggingface
            - name: jit-cache
              mountPath: /var/cache/vllm
            - name: devinf
              mountPath: /dev/infiniband
            - name: model-storage
              mountPath: /mnt/models
              readOnly: true
            - name: aiter-cache
              mountPath: /mnt/aiter-cache
