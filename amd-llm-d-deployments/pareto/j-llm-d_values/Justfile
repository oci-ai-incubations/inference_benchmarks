set dotenv-load
set dotenv-path := "../.env"

# Configs to sweep (space-separated). Each name maps to configs/<name>.yaml.
# Prefix determines base template: agg-* → glm-agg, disagg-* → glm-disagg.
export CONFIGS := env("CONFIGS", "disagg-3p5d-32 disagg-2p2d-24 disagg-2p1d-16 disagg-1p1d-8 agg-tp8 agg-tp4")

# Max GPUs to use concurrently (configs are scheduled to fit within this budget)
export MAX_GPUS := env("MAX_GPUS", "32")

# Model to deploy
export MODEL := env("MODEL", "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic")

# Sweep configuration
export ISL := env("ISL", "4096")
export OSL := env("OSL", "256")
export CONCURRENCIES := env("CONCURRENCIES", "4 8 12 16 20 24 28 32 40 48 56 64")
export TARGET_DURATION := env("TARGET_DURATION", "60")
export MIN_PROMPTS := env("MIN_PROMPTS", "100")
export IMAGE := env("IMAGE", "quay.io/tms/poker:0.0.13")

default:
  just --list

# --- Full lifecycle ---

# Run a single config end-to-end: deploy → wait → sweep → wait → teardown
run CONFIG:
  #!/usr/bin/env bash
  set -euo pipefail
  echo "=== Running config: {{CONFIG}} ==="
  just setup-ns "{{CONFIG}}"
  just deploy "{{CONFIG}}"
  just wait-ready "{{CONFIG}}"
  just sweep "{{CONFIG}}"
  just wait-sweep "{{CONFIG}}"
  just teardown "{{CONFIG}}"
  echo "=== Config {{CONFIG}} complete ==="

# Compute GPU count for a config from its YAML file
gpu-count CONFIG:
  #!/usr/bin/env bash
  python3 -c "
  import yaml, sys
  with open('configs/{{CONFIG}}.yaml') as f:
      d = yaml.safe_load(f)
  gpus = 0
  for role in ('decode', 'prefill'):
      if role in d:
          tp = d[role].get('parallelism', {}).get('tensor', 1)
          replicas = d[role].get('replicas', 1)
          gpus += tp * replicas
  print(gpus)
  "

# Run all configs, scheduling by GPU budget
run-all:
  #!/usr/bin/env zsh
  set -euo pipefail
  MAX={{MAX_GPUS}}
  CONFIGS=({{CONFIGS}})
  TOTAL=${#CONFIGS[@]}

  # Compute GPU counts upfront
  typeset -A GPU_COST
  for CFG in "${CONFIGS[@]}"; do
    GPU_COST[$CFG]=$(just gpu-count "$CFG")
  done

  echo "Running $TOTAL configs (max $MAX GPUs concurrently)"
  for CFG in "${CONFIGS[@]}"; do
    echo "  $CFG: ${GPU_COST[$CFG]} GPUs"
  done
  echo ""

  # GPU-budget scheduler
  typeset -A PID_CFG   # pid -> config name
  USED=0
  QUEUE=("${CONFIGS[@]}")
  FAILED=0

  # Kill all background processes on Ctrl-C / exit
  cleanup() {
    echo ""
    echo "Caught signal, killing running configs..."
    trap - INT TERM          # reset to default so kill 0 doesn't re-enter
    echo "Running deployments may still be up — use 'just teardown-all' to clean up."
    kill 0                   # kill entire process group (script + all children)
  }
  trap cleanup INT TERM

  while (( ${#QUEUE[@]} > 0 )) || (( ${#PID_CFG[@]} > 0 )); do
    # Launch configs that fit within budget (scan entire queue, not just head)
    LAUNCHED=true
    while $LAUNCHED && (( ${#QUEUE[@]} > 0 )); do
      LAUNCHED=false
      for (( I=1; I<=${#QUEUE[@]}; I++ )); do
        NEXT="${QUEUE[$I]}"
        NEED=${GPU_COST[$NEXT]}
        if (( USED + NEED <= MAX )); then
          echo ">>> Launching $NEXT (${NEED} GPUs, $((USED+NEED))/${MAX} used)"
          (just run "$NEXT" 2>&1 | awk -v c="$NEXT" '{print "["c"] "$0; fflush()}') &
          PID_CFG[$!]=$NEXT
          USED=$((USED + NEED))
          QUEUE[$I]=()
          LAUNCHED=true
          break
        fi
      done
    done

    # Wait for any running config to finish
    if (( ${#PID_CFG[@]} > 0 )); then
      while true; do
        for PID in "${(@k)PID_CFG}"; do
          if ! kill -0 "$PID" 2>/dev/null; then
            wait "$PID" || FAILED=$((FAILED + 1))
            CFG="${PID_CFG[$PID]}"
            COST=${GPU_COST[$CFG]}
            USED=$((USED - COST))
            unset "PID_CFG[$PID]"
            echo "<<< $CFG finished (freed ${COST} GPUs, ${USED}/${MAX} used)"
            break 2
          fi
        done
        sleep 5
      done
    fi
  done

  if (( FAILED > 0 )); then
    echo "WARNING: $FAILED config(s) failed"
  fi
  echo ""
  echo "=== All configs complete ==="
  just collect

# --- Namespace setup ---

# Create namespace with secrets and poker RBAC for a config
setup-ns CONFIG:
  #!/usr/bin/env bash
  set -euo pipefail
  NS="{{CONFIG}}"
  echo "Setting up namespace: $NS"
  kubectl create ns "$NS" --dry-run=client -o yaml | kubectl apply -f -
  # HF secret
  kubectl -n "$NS" create secret generic hf-secret \
    --from-literal=HF_TOKEN="$HF_TOKEN" \
    --dry-run=client -o yaml | kubectl apply -f -
  # Poker RBAC (SA + Role + RoleBinding, skip the Pod)
  kubectl -n "$NS" apply -f - <<'EOF'
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: poker
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    name: poker
  rules:
    - apiGroups: [""]
      resources: ["pods"]
      verbs: ["get", "list"]
    - apiGroups: ["gateway.networking.k8s.io"]
      resources: ["gateways"]
      verbs: ["get", "list"]
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    name: poker
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: poker
  subjects:
    - kind: ServiceAccount
      name: poker
  EOF

# --- Deploy / Teardown ---

# Deploy a config's model servers
deploy CONFIG:
  #!/usr/bin/env bash
  set -euo pipefail
  NS="{{CONFIG}}"
  if [[ "{{CONFIG}}" == disagg-* ]]; then
    BASE=glm-disagg
  else
    BASE=glm-agg
  fi
  MS_VALUES="$(cd .. && pwd)/glm-pareto/configs/{{CONFIG}}.yaml"
  echo "Deploying $NS (base=$BASE)"
  # Extract TP from config for NIC count (disagg: nvidia.com/sriov-rdma-vf matches TP)
  if [[ "$BASE" == glm-disagg ]]; then
    CONFIG_FILE="configs/{{CONFIG}}.yaml"
    DECODE_TP=$(python3 -c "import yaml; d=yaml.safe_load(open('$CONFIG_FILE')); print(d.get('decode',{}).get('parallelism',{}).get('tensor',8))")
    PREFILL_TP=$(python3 -c "import yaml; d=yaml.safe_load(open('$CONFIG_FILE')); print(d.get('prefill',{}).get('parallelism',{}).get('tensor',4))")
    echo "  NIC count: decode=$DECODE_TP prefill=$PREFILL_TP (matches TP)"
  fi
  cd "../$BASE"
  env MODEL="{{MODEL}}" MS_VALUES="$MS_VALUES" \
    DECODE_TP="${DECODE_TP:-8}" PREFILL_TP="${PREFILL_TP:-4}" \
    helmfile apply -n "$NS"
  kubectl -n "$NS" apply -f httproute.yaml

# Wait for model server pods to be ready
wait-ready CONFIG:
  #!/usr/bin/env bash
  set -euo pipefail
  NS="{{CONFIG}}"
  if [[ "{{CONFIG}}" == disagg-* ]]; then
    GUIDE=glm-disagg
  else
    GUIDE=glm-agg
  fi
  echo "Waiting for model servers in $NS..."
  # Wait for at least one pod to exist
  while [ "$(kubectl -n "$NS" get pods -l "llm-d.ai/guide=$GUIDE,llm-d.ai/inference-serving=true" -o name 2>/dev/null | wc -l)" -eq 0 ]; do
    sleep 10
  done
  kubectl -n "$NS" wait --for=condition=ready pods \
    -l "llm-d.ai/guide=$GUIDE,llm-d.ai/inference-serving=true" \
    --timeout=30m
  echo "Model servers ready in $NS"

# Tear down model servers (keeps namespace + sweep job pods for log collection)
teardown CONFIG:
  #!/usr/bin/env bash
  set -euo pipefail
  NS="{{CONFIG}}"
  if [[ "{{CONFIG}}" == disagg-* ]]; then
    BASE=glm-disagg
  else
    BASE=glm-agg
  fi
  echo "Tearing down model servers in $NS"
  cd "../$BASE"
  helmfile destroy -n "$NS" || true
  kubectl -n "$NS" delete -f httproute.yaml --ignore-not-found=true 2>/dev/null || true

# --- Sweep ---

# Launch sweep job for a config (model servers must already be running)
sweep CONFIG:
  #!/usr/bin/env bash
  set -euo pipefail
  NS="{{CONFIG}}"
  if [[ "{{CONFIG}}" == disagg-* ]]; then
    TARGET=glm-disagg
  else
    TARGET=glm-agg
  fi
  echo "Launching sweep job for {{CONFIG}} in namespace $NS"
  env \
    JOB_NAME="glm-pareto-{{CONFIG}}" \
    CONFIG_NAME="{{CONFIG}}" \
    TARGET="$TARGET" \
    NAMESPACE="$NS" \
    ISL={{ISL}} \
    OSL={{OSL}} \
    CONCURRENCY_LEVELS="{{CONCURRENCIES}}" \
    TARGET_DURATION={{TARGET_DURATION}} \
    MIN_PROMPTS={{MIN_PROMPTS}} \
    IMAGE="{{IMAGE}}" \
    envsubst '${JOB_NAME} ${CONFIG_NAME} ${TARGET} ${NAMESPACE} ${ISL} ${OSL} ${CONCURRENCY_LEVELS} ${TARGET_DURATION} ${MIN_PROMPTS} ${IMAGE}' \
      < job.yaml | kubectl -n "$NS" replace --force -f -

# Wait for a config's sweep job to complete
wait-sweep CONFIG:
  #!/usr/bin/env bash
  set -euo pipefail
  NS="{{CONFIG}}"
  echo "Waiting for sweep job in $NS..."
  kubectl -n "$NS" wait --for=condition=complete --timeout=3h \
    "job/glm-pareto-{{CONFIG}}" || \
    echo "WARNING: sweep job for {{CONFIG}} did not complete successfully"

# --- Results ---

# Collect results from all config namespaces
collect:
  #!/usr/bin/env bash
  set -euo pipefail
  NAMESPACES=""
  for CFG in {{CONFIGS}}; do
    NAMESPACES="$NAMESPACES -n $CFG"
  done
  python3 collect.py $NAMESPACES

# Collect results and upload to Google Sheets
collect-sheets TITLE="GLM Pareto Sweep":
  #!/usr/bin/env bash
  set -euo pipefail
  NAMESPACES=""
  for CFG in {{CONFIGS}}; do
    NAMESPACES="$NAMESPACES -n $CFG"
  done
  python3 collect.py $NAMESPACES --sheets "{{TITLE}}"

# --- Status & Cleanup ---

# Show status across all config namespaces
status:
  #!/usr/bin/env bash
  for CFG in {{CONFIGS}}; do
    echo "=== $CFG ==="
    kubectl -n "$CFG" get jobs -l app=glm-pareto 2>/dev/null || echo "  (namespace not found)"
    kubectl -n "$CFG" get pods -l llm-d.ai/inference-serving=true --no-headers 2>/dev/null || true
    echo ""
  done

# Delete sweep jobs across all namespaces (keeps model servers)
clean:
  #!/usr/bin/env bash
  for CFG in {{CONFIGS}}; do
    kubectl -n "$CFG" delete jobs -l app=glm-pareto --ignore-not-found=true 2>/dev/null || true
  done

# Tear down model servers in all namespaces
teardown-all:
  #!/usr/bin/env bash
  for CFG in {{CONFIGS}}; do
    just teardown "$CFG" || true
  done

# Delete all config namespaces entirely
destroy-all:
  #!/usr/bin/env bash
  for CFG in {{CONFIGS}}; do
    echo "Deleting namespace $CFG"
    kubectl delete ns "$CFG" --ignore-not-found=true 2>/dev/null || true
  done

# --- GPU–NIC Topology ---

# Dump GPU <-> NIC pairing for a running config. Requires model server pods to be up.
# From inside a pod: shows PCI topology and (when available) nvidia-smi topo or rocm-smi.
# GPU–NIC pairing matters for GPUDirect RDMA: devices sharing the same PCIe root complex
# (PIX/PXB in nvidia-smi topo) achieve best throughput.
gpu-nic-topo CONFIG="disagg-1p1d-8":
  #!/usr/bin/env bash
  set -euo pipefail
  NS="{{CONFIG}}"
  if [[ "{{CONFIG}}" == disagg-* ]]; then
    GUIDE=glm-disagg
  else
    GUIDE=glm-agg
  fi
  POD=$(kubectl -n "$NS" get pods -l "llm-d.ai/guide=$GUIDE,llm-d.ai/inference-serving=true" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
  if [ -z "$POD" ]; then
    echo "No model server pod in namespace $NS. Deploy first with: just deploy {{CONFIG}}"
    echo ""
    echo "For node-level topology without a pod, use:"
    echo "  kubectl debug node/<NODE> -it --image=ubuntu:22.04 -- chroot /host lspci -tv"
    echo "  kubectl debug node/<NODE> -it --image=nvidia/cuda:12-base -- chroot /host nvidia-smi topo -m"
    exit 1
  fi
  echo "=== GPU–NIC topology from pod $POD (node: $(kubectl -n "$NS" get pod "$POD" -o jsonpath='{.spec.nodeName}')) ==="
  echo ""
  echo "--- PCI tree (GPU + NIC placement) ---"
  kubectl -n "$NS" exec "$POD" -c vllm -- lspci -tv 2>/dev/null || kubectl -n "$NS" exec "$POD" -c vllm -- ls -la /sys/bus/pci/devices/ 2>/dev/null || echo "  (lspci not in image, try node debug)"
  echo ""
  echo "--- NVIDIA GPU topology (if NVIDIA GPUs) ---"
  kubectl -n "$NS" exec "$POD" -c vllm -- nvidia-smi topo -m 2>/dev/null || echo "  (nvidia-smi not in image — AMD/other GPU)"
  echo ""
  echo "--- AMD ROCm topology (if AMD GPUs) ---"
  kubectl -n "$NS" exec "$POD" -c vllm -- rocm-smi --showtoponuma 2>/dev/null || echo "  (rocm-smi not available)"
  echo ""
  echo "--- Assigned SR-IOV VFs (PCIDEVICE env) ---"
  kubectl -n "$NS" exec "$POD" -c vllm -- printenv PCIDEVICE_NVIDIA_COM_SRIOV_RDMA_VF 2>/dev/null || kubectl -n "$NS" exec "$POD" -c vllm -- printenv | grep -i pcidevice 2>/dev/null || echo "  (no SR-IOV VF assigned to this pod)"
  echo ""
  echo "Interpretation: PIX = same PCI switch (best), PXB = multi-bridge, NODE = same NUMA, SYS = cross-NUMA."

# Node-level topology (no model pods required). Run from a debug pod on the node.
# Example: just gpu-nic-topo-node 10.0.76.204
gpu-nic-topo-node NODE:
  #!/usr/bin/env bash
  echo "Run these on node {{NODE}} (e.g. via 'kubectl debug node/{{NODE}} -it --image=... -- bash'):"
  echo ""
  echo "  # PCI tree"
  echo "  lspci -tv"
  echo ""
  echo "  # NVIDIA GPU+NIC topology (PIX=best pairing)"
  echo "  nvidia-smi topo -m"
  echo ""
  echo "  # Full hwloc topology (install: apt-get -y install hwloc && lstopo --of console)"
  echo "  lstopo --of console"
  echo ""
  echo "One-liner debug pod:"
  echo "  kubectl debug node/{{NODE}} -it --image=ubuntu:22.04 --target=node -- chroot /host bash -c 'lspci -tv; echo; nvidia-smi topo -m 2>/dev/null || true'"
